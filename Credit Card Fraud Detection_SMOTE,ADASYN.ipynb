{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Starting:\n",
    "\n",
    "If you liked this kernel please don't forget to upvote the project, this will keep me motivated to other kernels in the future. I hope you enjoy our deep exploration into this dataset. Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Credit Card Fraud Detection**\n",
    "**Anonymized credit card transactions labeled as fraudulent or genuine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Introduction__\n",
    "\n",
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase. Eventually it is also important for companies NOT to detect transactions which are genuine as fradulent, otherwise companies whould keep blocking the credit card, and which may lead to customer dissatisfaction. So here are two important expects of this analysis:\n",
    "\n",
    "* What would happen when company will not able to detect the fradulent transation and would not confirm from customer about this recent transaction wheather it was made by him/her.\n",
    "\n",
    "* In contract, what would happen when company will detect a genuine transaction as fradulent and keep calling customer for confirmation or might block card.\n",
    "\n",
    "The datasets contains transactions that have 492 frauds out of 284,807 transactions. So the dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. When we try to build the prediction model with these kind of unbalanced dataset, then model will be more inclined towards to detect new unseen transaction as genuine as out dataset contains about 99% genuine data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import math\n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "# Print versions of libraries\n",
    "print(f\"Numpy version : Numpy {np.__version__}\")\n",
    "print(f\"Pandas version : Pandas {pd.__version__}\")\n",
    "print(f\"Matplotlib version : Matplotlib {matplotlib.__version__}\")\n",
    "print(f\"Seaborn version : Seaborn {sns.__version__}\")\n",
    "print(f\"SkLearn version : SkLearn {sklearn.__version__}\")\n",
    "\n",
    "# Magic Functions for In-Notebook Display\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting seabon style\n",
    "sns.set(style='darkgrid', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/creditcardfraud/creditcard.csv', encoding='latin_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all column names to lower case\n",
    "df.columns = df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Due to confidentiality issue, original features V1, V2,... V28 have been transformed with PCA, however we may guess that these features might be orginally credit card number, expirary date, CVV, card holder name, transaction location, transaction date time, etc.** \n",
    "\n",
    "* The only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. \n",
    "\n",
    "* Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customising default values to view all columns\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "# pd.set_option('display.max_rows',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect.getfullargspec(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**\n",
    "\n",
    "Once the data is read into python, we need to explore/clean/filter it before processing it for machine learning It involves adding/deleting few colums or rows, joining some other data, and handling qualitative variables like dates.\n",
    "\n",
    "Now that we have the data, I wanted to run a few initial comparisons between the three columns - Time, Amount, and Class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking concise summary of dataset\n",
    "\n",
    "It is also a good practice to know the features and their corresponding data types,along with finding whether they contain null values or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Highlights**\n",
    "\n",
    "* Dataset contains details of 284807 transactions with 31 features.\n",
    "* There is no missing data in out dataset, every columns contain excatly 284807 rows.\n",
    "* All data types are float64 ,except 1 : Class \n",
    "* All data types are float64 ,except 1 : Class \n",
    "* 28 columns have Sequential Names and values that don't make any logical sense - > V1 , V2 ....V28\n",
    "* 3 columns : TIME , AMOUNT and CLASS which can be analysed for various INSIGHTS ! \n",
    "* Memory Usage : 67 MB , not so Harsh !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count unique values of label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['class'].value_counts())\n",
    "print('\\n')\n",
    "print(df['class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"].value_counts().plot(kind = 'pie',explode=[0, 0.1],figsize=(6, 6),autopct='%1.1f%%',shadow=True)\n",
    "plt.title(\"Fraudulent and Non-Fraudulent Distribution\",fontsize=20)\n",
    "plt.legend([\"Fraud\", \"Genuine\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Highlights**\n",
    "\n",
    "This dataset have 492 frauds out of 284,807 transactions. The dataset is **highly unbalanced**, the positive class (frauds) account for 0.172% of all transactions. Most of the transactions are non-fraud. If we use this dataframe as the base for our predictive models and analysis, our algorithms will probably overfit since it will \"assume\" that most transactions are not fraud. But we don't want our model to assume, we want our model to detect patterns that give signs of fraud!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate descriptive statistics\n",
    "\n",
    "Lets summarize the central tendency, dispersion and shape of a dataset's distribution. Out of all the columns, the only ones that made the most sense were Time, Amount, and Class (fraud or not fraud). The other 28 columns were transformed using what seems to be a PCA dimensionality reduction in order to protect user identities.\n",
    "\n",
    "The data itself is short in terms of time (itâ€™s only 2 days long), and these transactions were made by European cardholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['time','amount','class']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Highlights**\n",
    "* On an average, credict card transaction is happening at every 94813.86 seconds.\n",
    "* Average transation amount is 88.35 with standard deviation of 250, with minimum amount of 0.0 and maximum amount 25,691.16. By seeing the 75% and maximum amount, it look like the feature 'Amount' is higly **positive skewed**. We will check the distribution graph of amount to get more clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with missing data\n",
    "df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Highlights**\n",
    "\n",
    "There are no missing values present in the dataset. It is not necessary that missing values are present in the dataset in the form of  NA, NAN, Zeroes etc, it may be present by some other values also that can be explored by analysising the each features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the duplicate data\n",
    "# ?print(\"No of duplicate data : \",len(df[df.duplicated()]))  \n",
    "# print(\"\\n\")\n",
    "print(\"Percentage of duplicate data : \",round(len(df[df.duplicated()])/len(df),4)*100, \"%\")  \n",
    "print(\"\\n\")\n",
    "print(\"Duplicate vs Non duplicate counts :\")\n",
    "print(df.duplicated().value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1081 duplicate rows present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the Duplicate Values\n",
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate data if they exist \n",
    "print(df.duplicated().value_counts())\n",
    "\n",
    "# Reset the index\n",
    "df.reset_index(drop = True , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True , drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transaction for zero amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['amount'] == 0]['amount'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['amount'] == 0) & (df['class'] == 1)]['amount'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is impossibile to have transation of amount zero from credit card. So these 1808 zero value transaction are actually null values and need to remove. However out of 1808 zero value transations, 25 are actually recognized as fradulent and rest as geninue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the zero value non-fraud transactions only\n",
    "Out data is highly unbalanced, and deleting the fraud transaction will make it more unbalanced.So we will delete only the genuine transactions of zero value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the zero value non-fraud transactions only\n",
    "df.drop(df[(df['amount'] == 0) & (df['class'] == 1)].index, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if zero transactions are removed or not\n",
    "df[(df['amount'] == 0) & (df['class'] == 1)]['amount'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True , drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Distribution of Transaction Amount', fontsize=14)\n",
    "sns.distplot(df['amount'], bins=100)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most the transaction amount falls between 0 and about 3000 and we have some outliers for really big amount transactions and it may actually make sense to drop those outliers in our analysis if they are just a few points that are very extreme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Amount for Fradulent & Genuine transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2,figsize=(16,4))\n",
    "sns.distplot(df[df['class'] == 1]['amount'], bins=100, ax=axs[0])\n",
    "axs[0].set_title(\"Distribution of Fraud Transactions\")\n",
    "\n",
    "sns.distplot(df[df['class'] == 0]['amount'], bins=100, ax=axs[0])\n",
    "axs[1].set_title(\"Distribution of Genuine Transactions\")\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows that most of the fraud transaction amount is less than 500 dollor. This also shows that the fraud transaction is very high for an amount near to 0, lets find that amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fraud Transaction distribution : \\n\",df[(df['class'] == 1)]['amount'].value_counts().head())\n",
    "print(\"\\n\")\n",
    "print(\"Maximum amount of fraud transaction - \",df[(df['class'] == 1)]['amount'].max())\n",
    "print(\"Minimum amount of fraud transaction - \",df[(df['class'] == 1)]['amount'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 105 fraud transactions for just one dollor and 27 fraud transaction for $99.99. And higest fraud transaction amount was 2125.87 and lowest was just 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of Amount w.r.t Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(x='class', y='amount',data = df)\n",
    "plt.title('Amount Distribution for Fraud and Genuine transactions')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most the transaction amount falls between 0 and about 3000 and we have some outliers for really big amount transactions and it may actually make sense to drop those outliers in our analysis if they are just a few points that are very extreme. Also we have should be conscious about that these **outlier should not be the fradulent transaction**. Generally, fradulent trasactions can of big amount and removing them from the data, can make the predicting model bais. \n",
    "\n",
    "So we can essentially build a model that realistically predicts transation as fraud without affected by outliers. It may not be really useful to actually have our model train on these extreme outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Distribution of Transaction Time', fontsize=14)\n",
    "sns.distplot(df['time'], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By seeing graph, we can see there are two peaks in the graph and even there are some local peaks. We can think of these as the time of the day, like the peak is the day time when most people do the transactions and the depth is the night time when most people just sleeps. We aleady know that out data contains credit card transaction for only two days, so there are two peaks for day time and one depth for one night time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of time w.r.t. transactions types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(16,4))\n",
    "\n",
    "sns.distplot(df[(df['class'] == 1)]['time'], bins=100, ax=axs[0])\n",
    "axs[0].set_title(\"Distribution of Fraud Transactions\")\n",
    "\n",
    "sns.distplot(df[(df['class'] == 0)]['time'], bins=100, ax=axs[1])\n",
    "axs[1].set_title(\"Distribution of Genue Transactions\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x='class', y='time',data = df)\n",
    "plt.title('Time Distribution for Fraud and Genuine transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of transaction type w.r.t amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2,sharex=True,figsize=(16,6))\n",
    "\n",
    "sns.scatterplot(x='time',y='amount', data=df[df['class']==1], ax=axs[0])\n",
    "axs[0].set_title(\"Distribution of Fraud Transactions\")\n",
    "\n",
    "sns.scatterplot(x='time',y='amount', data=df[df['class']==0], ax=axs[1])\n",
    "axs[1].set_title(\"Distribution of Genue Transactions\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3 = np.percentile(df['amount'], 75)\n",
    "Q1 = np.percentile(df['amount'], 25)\n",
    "\n",
    "# calculate interquartrile range - IQR = thirdQuartile - firstQuartile\n",
    "IQR = (Q3 - Q1)\n",
    "    \n",
    "# Usually we take scale value of 1.5 times IQR to calculate. But this scale depends on the distribution followed by the data. \n",
    "# Say if my data seem to follow exponential distribution then this scale would change.   \n",
    "# So I am taking sacle from 1.5 to 5.\n",
    "\n",
    "# Lower outlier boundry (LOB) / Lower Whisker \n",
    "LOB = Q1 - (IQR * 5.0)\n",
    "print(f\"Lower Whisker : {LOB}\")\n",
    "    \n",
    "# Upper outlier boundry (UOB) / Upper Whisker\n",
    "UOB = Q3 + (IQR * 5.0)\n",
    "print(f\"Upper Whisker : {UOB}\")\n",
    "\n",
    "amtAllOutliers = df[(df['amount'] < LOB) | (df['amount'] > UOB)]['amount']\n",
    "amtFrdOutliers = df[(df['class'] == 1) & ((df['amount'] < LOB) | (df['amount'] > UOB))]['amount']\n",
    "amtGenuOutliers = df[(df['class'] == 0) & ((df['amount'] < LOB) | (df['amount'] > UOB))]['amount']\n",
    "\n",
    "print('\\n')\n",
    "print(\"No of all type of transaction outliers : \", amtAllOutliers.count())\n",
    "print(\"No of fraud transaction outliers : \", amtFrdOutliers.count())\n",
    "print(\"No of genuine transaction outliers : \", amtGenuOutliers.count())\n",
    "print(\"Percentage of outliers : \", round((amtGenuOutliers.count()/len(df))*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are total number of 11,213 outliers out of which 3.94% are fradulent.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking proportion of data for fraud vs genuine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the balace of data including outliers\n",
    "print(\"Balace of data including outliers\")\n",
    "print(df['class'].value_counts(normalize=True))\n",
    "\n",
    "print('\\n')\n",
    "# Check the balance of data excluding outliers\n",
    "print(\"Balace of data excluding outliers\")\n",
    "print(df[(df['amount'] < LOB) | (df['amount'] > UOB)]['class'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have check the total number of outliers in Amount feature and how many of them are fradulents. We found that balance of fraud vs genuine is not impacted much, so we can remove the outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape before deleting outliers\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers\n",
    "df = df.drop(amtAllOutliers.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape after deleting outliers\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True , drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Amount Distribution after deleting outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2,figsize=(12,6))\n",
    "\n",
    "sns.distplot(df['amount'], bins=100, ax=axs[0])\n",
    "axs[0].set_title(\"Amount Distribution\")\n",
    "\n",
    "sns.boxplot(x='class', y='amount',data = df, ax=axs[1])\n",
    "axs[1].set_title(\"Dsitribution of Amount wrt Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical vs Continuous Features\n",
    "\n",
    "Finging unique values for each column to understand which column is categorical and which one is Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finging unique values for each column\n",
    "df[['time','amount','class']].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Among Explanatory Variables\n",
    "\n",
    "Having **too many features** in a model is not always a good thing because it might cause overfitting and worser results when we want to predict values for a new dataset. Thus, **if a feature does not improve your model a lot, not adding it may be a better choice.**\n",
    "\n",
    "Another important thing is **correlation. If there is very high correlation between two features, keeping both of them is not a good idea most of the time not to cause overfitting.** However, this does not mean that you must remove one of the highly correlated features. \n",
    "\n",
    "Lets find out top 10 features which are highly correlaed with price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['time','amount','class']].corr()['class'].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Pearson Correlation Matrix')\n",
    "sns.heatmap(df[['time', 'amount','class']].corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"viridis\",\n",
    "            linecolor='w',annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like that no features are highly correlated with any other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets check the data again after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Engineering** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering on Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting time from second to hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting time from second to hour\n",
    "df['time'] = df['time'].apply(lambda sec : (sec/3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating hour of the day\n",
    "df['hour'] = df['time']%24   # 2 days of data\n",
    "df['hour'] = df['hour'].apply(lambda x : math.floor(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating First and Second Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating First and Second day\n",
    "df['day'] = df['time']/24   # 2 days of data\n",
    "df['day'] = df['day'].apply(lambda x : 1 if(x==0) else math.ceil(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['time','hour','day','amount','class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fraud and Genuine transaction Day wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating fraud transaction daywise\n",
    "dayFrdTran = df[(df['class'] == 1)]['day'].value_counts()\n",
    "# calculating genuine transaction daywise\n",
    "dayGenuTran = df[(df['class'] == 0)]['day'].value_counts()\n",
    "# calculating total transaction daywise\n",
    "dayTran = df['day'].value_counts()\n",
    "\n",
    "print(\"No of transaction Day wise:\")\n",
    "print(dayTran)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"No of fraud transaction Day wise:\")\n",
    "print(dayFrdTran)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"No of genuine transactions Day wise:\")\n",
    "print(dayGenuTran)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Percentage of fraud transactions Day wise:\")\n",
    "print((dayFrdTran/dayTran)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total number of transaction on Day 1 was 1,38,355, out of which 242 was fraud and 1,38,113 was genuie. Fraud transation was 0.17% of total transaction on day 1.\n",
    "\n",
    "* Total number of transaction on Day 2 was 1,34,133, out of which 166 was fraud and 1,33,967 was genuie. Fraud transation was 0.12% of total transaction on day 2.\n",
    "\n",
    "* Most of the transaction including the fraud transaction happened on day 1.\n",
    "\n",
    "Lets see the above numbers in graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=3, figsize=(16,4))\n",
    "\n",
    "sns.countplot(df['day'], ax=axs[0])\n",
    "axs[0].set_title(\"Distribution of Total Transactions\")\n",
    "\n",
    "sns.countplot(df[(df['class'] == 1)]['day'], ax=axs[1])\n",
    "axs[1].set_title(\"Distribution of Fraud Transactions\")\n",
    "\n",
    "sns.countplot(df[(df['class'] == 0)]['day'], ax=axs[2])\n",
    "axs[2].set_title(\"Distribution of Genuine Transactions\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time plots \n",
    "fig , axs = plt.subplots(nrows = 1 , ncols = 2 , figsize = (15,8))\n",
    "\n",
    "sns.distplot(df[df['class']==0]['time'].values , color = 'green' , ax = axs[0])\n",
    "axs[0].set_title('Genuine Transactions')\n",
    "\n",
    "sns.distplot(df[df['class']==1]['time'].values , color = 'red' ,ax = axs[1])\n",
    "axs[1].set_title('Fraud Transactions')\n",
    "\n",
    "fig.suptitle('Comparison between Transaction Frequencies vs Time for Fraud and Genuine Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if we find any particular pattern between time ( in hours ) and Fraud vs Genuine Transactions\n",
    "\n",
    "plt.figure(figsize=(12,10))\n",
    "\n",
    "sns.distplot(df[df['class'] == 0][\"hour\"], color='g') # Genuine - green\n",
    "sns.distplot(df[df['class'] == 1][\"hour\"], color='r') # Fraudulent - Red\n",
    "\n",
    "plt.title('Fraud vs Genuine Transactions by Hours', fontsize=15)\n",
    "plt.xlim([0,25])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above graph shows that most of the Fraud transactions are happening at night time (0 to 7 hours) when most of the people are sleeping and Genuine transaction are happening during day time (9 to 21 hours).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['time','hour','day','amount','class']].groupby('hour').count()['class'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising Data for detecting any particular Pattern or Anomaly using Histogram Plots\n",
    "\n",
    "Finally visulaising all columns once and for all to observe any abnormality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize = (25,25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace = True , drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scale Amount Feature**\n",
    "\n",
    "* It is good idea to scale the data, so that the column(feature) with lesser significance might not end up dominating the objective function due to its larger range. like a column like age has a range between 0 to 80, but a column like salary has range from thousands to lakhs, hence, salary column will dominate to predict the outcome even if it may not be important.\n",
    "* In addition, features having different unit should also be scaled thus providing each feature equal initial weightage. Like Age in years and Sales in Dollars must be brought down to a common scale before feeding it to the ML algorithm\n",
    "* This will result in a better prediction model.\n",
    "\n",
    "\n",
    "**Scaling using the log** : There are two main reasons to use logarithmic scales in charts and graphs. \n",
    "* The first is to respond to skewness towards large values; i.e., cases in which one or a few points are much larger than the bulk of the data. \n",
    "* The second is to show percent change or multiplicative factors. \n",
    "\n",
    "**PCA Transformation**: The description of the data says that all the features went through a PCA transformation (Dimensionality Reduction technique) except for time and amount.\n",
    "\n",
    "**Scaling**: Keep in mind that in order to implement a PCA transformation features need to be previously scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# df['ScaledAmount'] = scaler.fit_transform(df[['Amount']])\n",
    "# df['ScaledAmount'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ScaledTime'] = scaler.fit_transform(df[['Time']])\n",
    "# df['ScaledTime'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['Time','ScaledTime','Amount','ScaledAmount','Class']].tail(10)\n",
    "# df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale amount by Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale amount by log\n",
    "df['amount_log'] = np.log(df.amount + 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the Amount Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler # importing a class from a module of a library\n",
    "\n",
    "ss = StandardScaler() # object of the class StandardScaler ()\n",
    "df['amount_scaled'] = ss.fit_transform(df['amount'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineering to a better visualization of the values\n",
    "plt.figure(figsize=(14,6))\n",
    "# Let's explore the Amount by Class and see the distribuition of Amount transactions\n",
    "plt.subplot(121)\n",
    "ax = sns.boxplot(x =\"class\",y=\"amount\",data=df)\n",
    "ax.set_title(\"Class x Amount\", fontsize=20)\n",
    "ax.set_xlabel(\"Is Fraud?\", fontsize=16)\n",
    "ax.set_ylabel(\"Amount\", fontsize = 16)\n",
    "\n",
    "plt.subplot(122)\n",
    "ax1 = sns.boxplot(x =\"class\",y=\"amount_log\", data=df)\n",
    "ax1.set_title(\"Class x Log Amount\", fontsize=20)\n",
    "ax1.set_xlabel(\"Is Fraud?\", fontsize=16)\n",
    "ax1.set_ylabel(\"Amount(Log)\", fontsize = 16)\n",
    "\n",
    "# plt.subplot(123)\n",
    "# ax1 = sns.boxplot(x =\"class\",y=\"amount_scaled\", data=df)\n",
    "# ax1.set_title(\"Class x Scaled Amount\", fontsize=20)\n",
    "# ax1.set_xlabel(\"Is Fraud?\", fontsize=16)\n",
    "# ax1.set_ylabel(\"Amount(Log)\", fontsize = 16)\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.6, top = 0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see a slightly difference in log amount of our two Classes. \n",
    "* The IQR of fraudulent transactions are higher than normal transactions, but normal transactions have highest values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Amount and Transaction Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to bin amounts first , but the problem is the skewness of the amount , still , let's try\n",
    "legit_list = df[df['class']==0]['amount'].describe().tolist()\n",
    "fraud_list = df[df['class']==1]['amount'].describe().tolist()\n",
    "pd.DataFrame(np.transpose(legit_list) , np.transpose(fraud_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.DataFrame([df[df['class']==0]['amount'].describe().to_dict() , df[df['class']==1]['amount'].describe().to_dict()])\n",
    "comp_df = comp_df.T\n",
    "comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df.columns = ['Legit' , 'Fraud']\n",
    "comp_df.plot(kind = 'barh' , figsize = (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['time','hour','day','amount','amount_log','amount_scaled','class']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Saving preprossed data as serialized files__\n",
    "* To deploy the predictive models built we save them along with the required data files as serialized file objects\n",
    "* We save cleaned and processed input data, tuned predictive models as files so that they can later be re-used/shared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the processed data \n",
    "\n",
    "# Lets save the processed data so that we can use it later without running the preprocessing technique again and again.\n",
    "# df.to_csv('elementary_data_processed.csv' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CreditCardFraudDataCleaned = df\n",
    "\n",
    "# Saving the Python objects as serialized files can be done using pickle library\n",
    "# Here let us save the Final Data set after all the transformations as a file\n",
    "with open('CreditCardFraudDataCleaned.pkl', 'wb') as fileWriteStream:\n",
    "    pickle.dump(CreditCardFraudDataCleaned, fileWriteStream)\n",
    "    # Don't forget to close the filestream!\n",
    "    fileWriteStream.close()\n",
    "    \n",
    "print('pickle file is saved at Location:',os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a Pickle file\n",
    "with open('CreditCardFraudDataCleaned.pkl', 'rb') as fileReadStream:\n",
    "    CreditCardFraudDataFromPickle = pickle.load(fileReadStream)\n",
    "    # Don't forget to close the filestream!\n",
    "    fileReadStream.close()\n",
    "    \n",
    "# Checking the data read from pickle file. It is exactly same as the DiamondPricesData\n",
    "df = CreditCardFraudDataFromPickle\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('elementary_data_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Splitting data into Training and Testing samples**\n",
    "\n",
    "We dont use the full data for creating the model. Some data is randomly selected and kept aside for checking how good the model is. This is known as Testing Data and the remaining data is called Training data on which the model is built. Typically 70% of data is used as Training data and the rest 30% is used as Tesing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Target Variable and Predictor Variables\n",
    "X = df.drop(['time','class','hour','day','amount','amount_log','amount_scaled'],axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the library for splitting the data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check with the shapes of Training and testing datasets\n",
    "print(\"X_train - \",X_train.shape)\n",
    "print(\"y_train - \",y_train.shape)\n",
    "print(\"X_test - \",X_test.shape)\n",
    "print(\"y_test - \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Baseline for models__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Discuss Next Steps - \n",
    "\n",
    "1  __Classification Models__\n",
    "\n",
    "- Logistic Regression\n",
    "- XG Boost\n",
    "- SVM 's\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "\n",
    "2  __Class Imbalance Solutions__\n",
    "\n",
    "- Under Sampling\n",
    "- Over Sampling\n",
    "- SMOTE\n",
    "- ADASYN\n",
    "\n",
    "3  __Metrics__\n",
    "\n",
    "- Accuracy Score\n",
    "- Confusion Matrix\n",
    "- ROC_AUC\n",
    "- F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Model Building__\n",
    "\n",
    "##### We are aware that our dataset is highly imbalanced, however we check the performance of imbalance dataset first and later we implement some techniques to balance the dataset and again check the performance of balanced dataset. Finally we will compare each regression models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __1. Logistic Regression__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Logistic Regression with __imbalanced__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # Importing Classifier Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0) # Sequence for splitting\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs') # () towards the end\n",
    "logreg.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict from Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy : \",metrics.accuracy_score(y_pred , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values counts for fraud and genuine of test dataset\n",
    "pd.Series(y_pred).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our model predicted 76 transaction as fraud and 81671 transactions as genuine from test dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual values counts for fraud and genuine of test dataset\n",
    "pd.Series(y_test).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are originally 121 fraud transactions and our model predicted only 76 fraud transaction. So the accuracy of our model should be ${76}\\over{121}$, right?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "76/121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 62.81% should be our accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However, this not the case. Actually there are originally 121 fraud transactions and 81626 genuine transactions in test dataset. However our model predicted only 76 fraud transaction. Also it should be keep in mind that these 76 predicted fraud transaction may not be identified correctly. It means that these predicted 76 fraud transactions are NOT only from 121 originall fraud transaction, however they may be from genuine transactions as well.**\n",
    "\n",
    "We will see our real accuracy in below cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Model Evolution Matrix__\n",
    "\n",
    "## Confusion Matrix\n",
    "\n",
    "__Why and When__?\n",
    "\n",
    "__Every problem is different and derives a different set of values for a particular business use case , thus every model must be evaluated differently.__\n",
    "\n",
    "## Let's get to know the terminology and Structure first\n",
    "\n",
    "A confusion matrix is defined into four parts : __{ TRUE , FALSE } (Actual) ,{POSITIVE , NEGATIVE} (Predicted)__\n",
    "Positive and Negative is what you predict , True and False is what you are told\n",
    "\n",
    "Which brings us to 4 relations : True Positive , True Negative , False Positive , False Negative <br>\n",
    "__P__ redicted - __R__ ows and __A__ ctual as __C__ olumns <br>\n",
    "\n",
    "<img src = 'https://github.com/dktalaicha/Kaggle/blob/master/CreditCardFraudDetection/images/final_cnf.png?raw=true'>\n",
    "\n",
    "![](https://imgur.com/om1pl02)\n",
    "\n",
    "\n",
    "## __Accuracy , Precision and Recall__\n",
    "\n",
    "##### __Accuracy__ : The most used and classic classification metric : Suited for binary classification problems.\n",
    "\n",
    "$$  \\text{Accuracy} = \\frac{( TP + TN ) }{ (TP + TN + FP + FN )}$$\n",
    "\n",
    "Basically Rightly predicted results amongst all the results , used when the classes are balanced\n",
    "\n",
    "##### __Precision__ : What proportion of predicted positives are truly positive ? Used when we need to predict the positive thoroughly, sure about it !\n",
    "\n",
    "$$ \\text{Precision} = \\frac{( TP )}{( TP + FP )} $$\n",
    "\n",
    "##### __Sensitivity or Recall__ : What proportion of actual positives is correctly classified ? choice when we want to capture as many positives as possible\n",
    "\n",
    "$$ \\text{Recall} = \\frac{(TP)}{( TP + FN )} $$\n",
    "\n",
    "##### __F1 Score__ : Harmonic mean of Precision and Recall. It basically maintains a balance between the precision and recall for your classifier\n",
    "\n",
    "$$ F1 = \\frac{2 * (\\text{ precision } * \\text{ recall })}{(\\text{ precision } + \\text{ recall } )} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-dbca97f40871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcnf_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test,y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "# ax= plt.subplot()\n",
    "\n",
    "p = sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 61 transaction recognised as True Postive, means they are orignally fraud transactions and our model precited them as fraud.**\n",
    "\n",
    "**True Negative** - 81611 (truely saying negative - genuine transaction correctly identified as genuine)\n",
    "\n",
    "**True Postive** - 61 (truely saying positive - fraud transaction correctly identified as fraud)\n",
    "\n",
    "**False Negative** - 60 ( falsely saying negative - fraud transaction incorrectly identified as genuine)\n",
    "\n",
    "**False Positive** - 15 ( falsely saying positive - genuine transaction incorrectly identified as fraud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We already know that we have 121 fraud transaction in our test dataset, but our model predicted only 61 fraud transaction. So the real accuracy of our model is ${61}\\over{121}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "61/121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, **50.41%** is real accuracny of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __ROC AUC Curve__\n",
    "\n",
    "It is an evaluation metric that helps identify the strength of the model to distinguish between two outcomes. It defines if a model can create a clear boundary between the postive and the negative class. \n",
    "\n",
    "Let's talk about some definitions first: \n",
    "\n",
    "##### __Sensitivity__ or __Recall__\n",
    "\n",
    "The sensitivity of a model is defined by the proportion of actual positives that are classified as Positives , i.e = TP / ( TP + FN )\n",
    "\n",
    "$$ \\text{Recall or Sensitivity} = \\frac{(TP)}{( TP + FN )} $$\n",
    "\n",
    "<img src = \"https://github.com/dktalaicha/Kaggle/blob/master/CreditCardFraudDetection/images/sens.png?raw=true\">\n",
    "\n",
    "##### __Specificity__\n",
    "\n",
    "The specificity of a model is defined by the proportion of actual negatives that are classified as Negatives , i.e = TN / ( TN + FP )\n",
    "\n",
    "$$ \\text{Specificity} = \\frac{(TN)}{( TN + FP )} $$\n",
    "\n",
    "<img src = \"https://github.com/dktalaicha/Kaggle/blob/master/CreditCardFraudDetection/images/spec.png?raw=true\">\n",
    "\n",
    "As we can see that both are independent of each other and lie in teo different quadrants , we can understand that they are inversely related to each other. Thus as Sensitivity goes up , Specificity goes down and vice versa.\n",
    "\n",
    "### ROC CURVE\n",
    "\n",
    "It is a plot between Sesitivity and ( 1 - Specificity ) , which intuitively is a plot between True Positive Rate and False Positive Rate. \n",
    "It depicts if a model can clearly identify each class or not\n",
    "\n",
    "Higher the area under the curve , better the model and it's ability to seperate the positive and negative class.\n",
    "\n",
    "<img src = \"https://github.com/dktalaicha/Kaggle/blob/master/CreditCardFraudDetection/images/tpfpfntn.jpeg?raw=true\">\n",
    "<img src = \"https://github.com/dktalaicha/Kaggle/blob/master/CreditCardFraudDetection/images/auc.png?raw=true\">\n",
    "<img src = \"https://github.com/dktalaicha/Kaggle/blob/master/CreditCardFraudDetection/images/auc2.png?raw=true\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.roc_auc_score(y_test , y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logreg.predict_proba(X_test)\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC - \",auc,\"\\n\")\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Class Imbalance__\n",
    "\n",
    "Let's Fix the class Imbalance and apply some sampling techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under Sampling and Over Sampling\n",
    "\n",
    "<img src = 'https://github.com/dktalaicha/Kaggle/blob/master/CreditCardFraudDetection/images/under_over_sampling.jpg?raw=true'>\n",
    "\n",
    "# Synthetic Minority OverSampling Technique (SMOTE)\n",
    "<img src='https://github.com/dktalaicha/Kaggle/blob/master/CreditCardFraudDetection/images/smote.png?raw=true'>\n",
    "\n",
    "# ADASYN \n",
    "\n",
    "fdsfsfsdgdfsgds\n",
    "fgdsgdfgdfgfd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import imbalace technique algorithims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import imbalace technique algorithims\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.Logistic Regression with __Undersampling__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # counter takes values returns value_counts dictionary\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_res, y_res = rus.fit_resample(X, y)\n",
    "\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)\n",
    "\n",
    "# Undersampling with Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "# Accuracy is surely reduced , let's look at the roc curve now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC - \",auc,\"\\n\")\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.Logistic Regression with __Oversampling__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original dataset shape %s' % Counter(y))\n",
    "random_state = 42\n",
    "\n",
    "ros = RandomOverSampler(random_state=random_state)\n",
    "X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)\n",
    "\n",
    "# Oversampling with Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC - \",auc,\"\\n\")\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a breast cancer classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Logistic Regression with __SMOTE__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE, ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)\n",
    "\n",
    "# SMOTE Sampling with Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC - \",auc,\"\\n\")\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a breast cancer classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Logistic Regression with __ADASYN__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "X_res, y_res = adasyn.fit_resample(X, y)\n",
    "print('Resampled dataset shape %s' % Counter(y_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=0)\n",
    "\n",
    "#  ADASYN Sampling with Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "print(\"AUC - \",auc,\"\\n\")\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a breast cancer classifier')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "##### reduce 29 columns - 2 columns , so that I can look at them in a plot !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # SVD , t-SNE , Linear Discrimant Analysis\n",
    "X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f,ax = plt.figure(figsize=(24,6))\n",
    "\n",
    "plt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y_res== 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "plt.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y_res == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building different models with different balanced datasets \n",
    "Let's now try either different models , first by creating multiple datsets for undersampled , oversampled and SMOTE sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Undersampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_under, y_under = rus.fit_resample(X, y)\n",
    "print('Resampled dataset shape %s' % Counter(y_under))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Oversampled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_over, y_over = ros.fit_resample(X, y)\n",
    "print('Resampled dataset shape %s' % Counter(y_over))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SMOTE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "print('Resampled dataset shape %s' % Counter(y_smote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ADASYN Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original dataset shape %s' % Counter(y))\n",
    "\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_adasyn, y_adasyn = adasyn.fit_resample(X, y)\n",
    "print('Resampled dataset shape %s' % Counter(y_adasyn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now applying different models and evaluating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # Support Vector Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Classifier - Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Decision Tree Classifier with __imbalanced__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "dte = DecisionTreeClassifier()\n",
    "dte.fit( X_train, y_train )\n",
    "\n",
    "y_pred = dte.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Decision Tree Classifier with __Undersampling__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampled data with Decision Tree Classifiers\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=0.3, random_state=0)\n",
    "\n",
    "dte = DecisionTreeClassifier()\n",
    "dte.fit( X_train, y_train )\n",
    "\n",
    "y_pred = dte.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Decision Tree Classifier with __Oversampling__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampled data with Decision Tree Classifiers # Best model after Classifier - DTE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3, random_state=0)\n",
    "\n",
    "dte = DecisionTreeClassifier()\n",
    "dte.fit( X_train, y_train )\n",
    "\n",
    "y_pred = dte.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Decision Tree Classifier with __SMOTE__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE data with Decision Tree Classifiers\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=0)\n",
    "\n",
    "dte = DecisionTreeClassifier()\n",
    "dte.fit( X_train, y_train )\n",
    "\n",
    "y_pred = dte.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Random Forest Classifier with __imbalance__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit( X_train, y_train )\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest Classifier with __Undersampling__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampled data with Decision Tree Classifiers\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=0.3, random_state=0)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit( X_train, y_train )\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Random Forest Classifier with __Oversampling__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampled data with Decision Tree Classifiers # Best model after Classifier - DTE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3, random_state=0)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit( X_train, y_train )\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Random Forest Classifier with __SMOTE__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE data with Decision Tree Classifiers\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=0)\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit( X_train, y_train )\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "# # 4.2 Build Models\n",
    "# # Letâ€™s test 6 different algorithms:\n",
    "\n",
    "# # Logistic Regression (LR)\n",
    "# # Linear Discriminant Analysis (LDA)\n",
    "# # K-Nearest Neighbors (KNN).\n",
    "# # Classification and Regression Trees (CART).\n",
    "# # Gaussian Naive Bayes (NB).\n",
    "# # Support Vector Machines (SVM).\n",
    "\n",
    "# # Spot Check Algorithms\n",
    "# models = []\n",
    "# models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "# models.append(('DT', DecisionTreeClassifier()))\n",
    "# models.append(('RF', RandomForestClassifier()))\n",
    "# models.append(('SVM', SVC(gamma='auto')))\n",
    "# models.append(('KNN', KNeighborsClassifier()))\n",
    "\n",
    "# # evaluate each model in turn\n",
    "# results = []\n",
    "# names = []\n",
    "\n",
    "# for name, model in models:\n",
    "#     kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "#     cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "#     results.append(cv_results)\n",
    "#     names.append(name)\n",
    "#     print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))\n",
    "    \n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 K Nearest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 KNN Classifier with __imbalance__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit( X_train, y_train )\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 KNN Classifier with __Undersampling__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampled data with Decision Tree Classifiers\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_under, y_under, test_size=0.3, random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit( X_train, y_train )\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 KNN with __Oversampling__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampled data with Decision Tree Classifiers # Best model after Classifier - DTE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_over, y_over, test_size=0.3, random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit( X_train, y_train )\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 KNN Classifier with __SMOTE__ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE data with Decision Tree Classifiers\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=0)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit( X_train, y_train )\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print(\"Accuracy :\",metrics.accuracy_score(y_pred , y_test))  \n",
    "print(\"AUC : \",metrics.roc_auc_score(y_test , y_pred))\n",
    "\n",
    "# plot ROC Curve\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, y_pred)\n",
    "\n",
    "plt.plot(fpr,tpr,linewidth=2, label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.plot([0,1], [0,1], 'k--' )\n",
    "\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.title('ROC curve for Predicting a credit card fraud detection')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap for Confusion Matrix\n",
    "\n",
    "cnf_matrix = metrics.confusion_matrix(y_test , y_pred)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, annot_kws={\"size\": 25}, cmap=\"YlGnBu\" ,fmt='g')\n",
    "\n",
    "plt.title('Confusion matrix', y=1.1, fontsize = 22)\n",
    "plt.xlabel('Predicted',fontsize = 18)\n",
    "plt.ylabel('Actual',fontsize = 18)\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['Genuine', 'Fraud']); \n",
    "# ax.yaxis.set_ticklabels(['Genuine', 'Fraud']);\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
